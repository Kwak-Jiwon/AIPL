# -*- coding: utf-8 -*-
"""MidiNet_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dwxYBEte3sX5boVQneu0r3iXHifRgtPU
"""

!pip install tensorflow matplotlib

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils import shuffle

!pip install mido

!pip install mido pretty_midi

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
import pretty_midi
import pandas as pd
from typing import Optional

# MIDI 파일을 피아노 롤로 변환하는 함수
def midi_to_piano_roll(midi_path, fs=16):
    midi_data = pretty_midi.PrettyMIDI(midi_path)
    piano_roll = midi_data.get_piano_roll(fs=fs)
    piano_roll = np.clip(piano_roll, 0, 1)
    piano_roll = piano_roll.T / np.max(piano_roll)  # 스케일링 추가
    return piano_roll

# 피아노 롤을 MIDI 파일로 변환하고 저장하는 함수 (노트 길이 10배 증가)
def piano_roll_to_midi(piano_roll, midi_path, fs=16, length_factor=10):
    midi = pretty_midi.PrettyMIDI()
    instrument = pretty_midi.Instrument(program=0)  # Acoustic Grand Piano

    if piano_roll.shape[1] != 128:
        piano_roll = piano_roll.T  # (128, time_steps)로 변환

    for pitch, row in enumerate(piano_roll):
        note_on = None
        for time, value in enumerate(row):
            if value > 0.5 and note_on is None:
                note_on = time
            elif value <= 0.5 and note_on is not None:
                note = pretty_midi.Note(
                    velocity=100,
                    pitch=pitch,
                    start=note_on / fs * length_factor,
                    end=time / fs * length_factor
                )
                instrument.notes.append(note)
                note_on = None
        if note_on is not None:
            note = pretty_midi.Note(
                velocity=100,
                pitch=pitch,
                start=note_on / fs * length_factor,
                end=(time + 1) / fs * length_factor
            )
            instrument.notes.append(note)

    midi.instruments.append(instrument)
    midi.write(midi_path)
    print(f"Saved generated MIDI: {midi_path}")

# 피아노 롤 시각화 함수
def plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None, title: str = "Piano Roll"):
    if count:
        title = f'First {count} notes'
    else:
        title = f'Whole track'
        count = len(notes['pitch'])
    plt.figure(figsize=(20, 4))
    plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)
    plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)
    plt.plot(
        plot_start_stop[:, :count], plot_pitch[:, :count], color="b", marker=".")
    plt.xlabel('Time [s]')
    plt.ylabel('Pitch')
    _ = plt.title(title)
    plt.show()

# 손실 값 시각화 함수
def plot_loss(d_losses, g_losses):
    plt.figure(figsize=(10, 5))
    plt.plot(d_losses, label='Discriminator Loss')
    plt.plot(g_losses, label='Generator Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

# Optimizer를 설정하는 함수 (학습률 조정)
def build_optimizer():
    initial_learning_rate = 0.0001  # <-- 학습률을 0.0001로 수정
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=initial_learning_rate,
        decay_steps=10000,
        decay_rate=0.9)
    return tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.5)

# MidiNet 모델 정의
class MidiNet(tf.keras.Model):
    def __init__(self, batch_size=64, output_w=32, output_h=128, y_dim=10, prev_dim=1, z_dim=256, gf_dim=128, df_dim=128,
                 gfc_dim=2048, dfc_dim=2048, c_dim=1):
        super(MidiNet, self).__init__()
        self.batch_size = batch_size
        self.output_w = output_w  # 더 긴 출력 길이
        self.output_h = output_h
        self.y_dim = y_dim
        self.prev_dim = prev_dim
        self.z_dim = z_dim  # <-- 노이즈 벡터 크기를 256으로 수정
        self.gf_dim = gf_dim  # <-- 생성자의 필터 크기를 128로 수정
        self.df_dim = df_dim  # <-- 판별자의 필터 크기를 128로 수정
        self.gfc_dim = gfc_dim  # 생성자 fully-connected layer 크기
        self.dfc_dim = dfc_dim  # 판별자 fully-connected layer 크기
        self.c_dim = c_dim

        self.build_model()

    def build_model(self):
        self.g_prev_bn = [tf.keras.layers.BatchNormalization() for _ in range(4)]
        self.g_bn = [tf.keras.layers.BatchNormalization() for _ in range(5)]
        self.d_bn = [tf.keras.layers.BatchNormalization() for _ in range(3)]

        self.d_optimizer = build_optimizer()
        self.g_optimizer = build_optimizer()

    def generator(self, z, y, prev_x):
        h0_prev = tf.keras.layers.LeakyReLU()(self.g_prev_bn[0](tf.keras.layers.Conv2D(16, (1, 128), strides=(1, 2), padding='same')(prev_x)))
        h1_prev = tf.keras.layers.LeakyReLU()(self.g_prev_bn[1](tf.keras.layers.Conv2D(16, (2, 1), padding='same')(h0_prev)))
        h2_prev = tf.keras.layers.LeakyReLU()(self.g_prev_bn[2](tf.keras.layers.Conv2D(16, (2, 1), padding='same')(h1_prev)))
        h3_prev = tf.keras.layers.LeakyReLU()(self.g_prev_bn[3](tf.keras.layers.Conv2D(16, (2, 1), padding='same')(h2_prev)))

        y = tf.cast(y, tf.float32)
        yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])
        z = tf.concat([z, y], axis=-1)

        h0 = tf.keras.layers.Dense(1024)(z)
        h0 = tf.concat([h0, y], axis=-1)
        h1 = tf.keras.layers.Dense(self.gf_dim * 2 * 2 * 1)(h0)

        h1 = tf.keras.layers.Reshape((2, 1, self.gf_dim * 2))(h1)
        yb_h1 = tf.tile(yb, [1, h1.shape[1], h1.shape[2], 1])
        h1 = tf.concat([h1, yb_h1, tf.image.resize(h3_prev, (h1.shape[1], h1.shape[2]))], axis=-1)

        h2 = tf.keras.layers.LeakyReLU()(self.g_bn[2](tf.keras.layers.Conv2DTranspose(self.gf_dim * 2, (2, 1), strides=(2, 2), padding='same')(h1)))
        yb_h2 = tf.tile(yb, [1, h2.shape[1], h2.shape[2], 1])
        h2 = tf.concat([h2, yb_h2, tf.image.resize(h2_prev, (h2.shape[1], h2.shape[2]))], axis=-1)

        h3 = tf.keras.layers.LeakyReLU()(self.g_bn[3](tf.keras.layers.Conv2DTranspose(self.gf_dim * 2, (2, 1), strides=(2, 2), padding='same')(h2)))
        yb_h3 = tf.tile(yb, [1, h3.shape[1], h3.shape[2], 1])
        h3 = tf.concat([h3, yb_h3, tf.image.resize(h1_prev, (h3.shape[1], h3.shape[2]))], axis=-1)

        h4 = tf.keras.layers.LeakyReLU()(self.g_bn[4](tf.keras.layers.Conv2DTranspose(self.gf_dim * 2, (2, 1), strides=(2, 2), padding='same')(h3)))
        yb_h4 = tf.tile(yb, [1, h4.shape[1], h4.shape[2], 1])
        h4 = tf.concat([h4, yb_h4, tf.image.resize(h0_prev, (h4.shape[1], h4.shape[2]))], axis=-1)

        return tf.keras.layers.Conv2DTranspose(self.c_dim, (1, 128), strides=(1, 2), activation='sigmoid', padding='same')(h4)

    def discriminator(self, x, y):
        y = tf.cast(y, tf.float32)
        yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])
        yb_x = tf.tile(yb, [1, x.shape[1], x.shape[2], 1])
        x = tf.concat([x, yb_x], axis=-1)
        h0 = tf.keras.layers.LeakyReLU()(tf.keras.layers.Conv2D(self.c_dim + self.y_dim, (2, 128), padding='same')(x))
        h0 = tf.concat([h0, yb_x], axis=-1)
        fm = h0
        h1 = tf.keras.layers.LeakyReLU()(self.d_bn[1](tf.keras.layers.Conv2D(self.df_dim + self.y_dim, (4, 1), padding='same')(h0)))
        h1 = tf.keras.layers.Flatten()(h1)
        h1 = tf.concat([h1, y], axis=-1)
        h2 = tf.keras.layers.LeakyReLU()(self.d_bn[2](tf.keras.layers.Dense(self.dfc_dim)(h1)))
        h2 = tf.concat([h2, y], axis=-1)
        h3 = tf.keras.layers.Dense(1)(h2)
        return tf.nn.sigmoid(h3), h3, fm

    def train_step(self, images, prev_images, labels, batch_z):
        with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:
            G = self.generator(batch_z, labels, prev_images)
            D_real, D_logits_real, _ = self.discriminator(images, labels)
            D_fake, D_logits_fake, _ = self.discriminator(G, labels)

            d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_real, labels=0.9 * tf.ones_like(D_real)))
            d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake, labels=tf.zeros_like(D_fake)))
            d_loss = d_loss_real + d_loss_fake

            g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_fake, labels=tf.ones_like(D_fake)))

        d_gradients = d_tape.gradient(d_loss, self.trainable_variables)
        g_gradients = g_tape.gradient(g_loss, self.trainable_variables)
        self.d_optimizer.apply_gradients(zip(d_gradients, self.trainable_variables))
        self.g_optimizer.apply_gradients(zip(g_gradients, self.trainable_variables))

        return d_loss, g_loss, G

# MIDI 파일을 불러와서 피아노 롤로 변환
midi_path = '/content/FlyMeToTheMoon.mid'  # MIDI 파일 경로
piano_roll = midi_to_piano_roll(midi_path)

# 훈련 데이터를 위한 전처리
target_shape = 16 * 128  # 리쉐이프하려는 목표 크기
length = (piano_roll.shape[0] // target_shape) * target_shape  # 정확한 배수로 자르기
piano_roll = piano_roll[:length]

# prev_X와 data_X 생성
prev_X = piano_roll.reshape(-1, 16, 128, 1)[:-1]
data_X = piano_roll.reshape(-1, 16, 128, 1)[1:]

# 레이블을 0으로 채우기
data_y = np.zeros((data_X.shape[0], 10), dtype=np.float32)  # 레이블을 0으로 채움

# 데이터 섞기
data_X, prev_X, data_y = shuffle(data_X, prev_X, data_y, random_state=0)

# 훈련 설정
batch_size = 64  # <-- 배치 크기를 64로 수정
epochs = 100  # 학습을 조금 더 오래 진행하도록 에포크 수 증가
batch_idxs = len(data_X) // batch_size

# 모델 인스턴스 생성
model = MidiNet()

# 샘플 입력 생성
sample_z = np.random.normal(0, 1, (batch_size, model.z_dim)).astype(np.float32)  # <-- 노이즈 벡터 크기 수정
sample_prev = prev_X[:batch_size]
sample_labels = data_y[:batch_size]

# 손실 값 기록을 위한 리스트
d_losses = []
g_losses = []

# 훈련 루프
for epoch in range(epochs):
    epoch_d_loss = 0
    epoch_g_loss = 0

    for idx in range(0, batch_idxs):
        batch_images = data_X[idx * batch_size:(idx + 1) * batch_size]
        prev_batch_images = prev_X[idx * batch_size:(idx + 1) * batch_size]
        batch_labels = data_y[idx * batch_size:(idx + 1) * batch_size]
        batch_z = np.random.normal(0, 1, [batch_size, model.z_dim]).astype(np.float32)  # <-- 노이즈 벡터 크기 수정

        d_loss, g_loss, _ = model.train_step(batch_images, prev_batch_images, batch_labels, batch_z)

        if idx % 10 == 0:
            print(f"Epoch: [{epoch + 1}/{epochs}], Step: [{idx}/{batch_idxs}], D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}")

        epoch_d_loss += d_loss
        epoch_g_loss += g_loss

    d_losses.append(epoch_d_loss / batch_idxs)
    g_losses.append(epoch_g_loss / batch_idxs)

    generated_samples = model.generator(sample_z, sample_labels, sample_prev)
    generated_samples = generated_samples.numpy()

    if epoch % 10 == 0:
        num_samples_to_save = min(len(generated_samples), 5)
        for i, extended_sample in enumerate(generated_samples[:num_samples_to_save]):
            extended_sample_2d = extended_sample[:, :, 0] if extended_sample.ndim == 3 else extended_sample
            midi_filename = f'/content/extended_sample_epoch_{epoch + 1}_sample_{i + 1}.mid'
            piano_roll_to_midi(extended_sample_2d, midi_filename, length_factor=10)

# 학습 완료 후 그래프 그리기
print("Training completed.")
plot_loss(d_losses, g_losses)

# 마지막 에포크에서 생성된 샘플을 시각화
num_samples_to_visualize = min(len(generated_samples), 5)

for i in range(num_samples_to_visualize):
    sample_piano_roll = generated_samples[i][:, :, 0]
    sample_piano_roll = (sample_piano_roll > 0.5).astype(int)

    pitches, positions = np.where(sample_piano_roll == 1)
    notes_df = pd.DataFrame({
        'pitch': pitches,
        'start': positions * 0.1,  # 임의의 시간 스케일링
        'end': (positions + 1) * 0.1
    })

    plot_piano_roll(notes_df, title=f'Final Sample {i + 1}')



